import json
import argparse
import logging
from collections import defaultdict
import os.path

# Set up logging for better visibility
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def calculate_accuracy_from_file(input_file: str, input_length_file: str = None, infer_mode: str = None):
    """
    Reads a JSONL file containing inference results and calculates accuracy metrics.
    
    For scoring mode (default):
      - Calculates traditional accuracy (all chosen > all rejected)
      - Calculates binary accuracy (chosen_score > rejected_score for each pair)
    
    For selection mode:
      - Calculates selection accuracy (whether the model selected the chosen response)

    Args:
        input_file: Path to the input JSONL file generated by the inference script.
        input_length_file: Optional path to a file containing validity flags for each line.
        infer_mode: The inference mode used to generate the results. If None, it will be 
                   auto-detected based on the file content.
    """
    logger.info(f"Starting accuracy calculation from '{input_file}'.")
    
    # Try to auto-detect the inference mode if not provided
    if infer_mode is None:
        with open(input_file, 'r', encoding='utf-8') as infile:
            for line in infile:
                try:
                    data = json.loads(line.strip())
                    if "chosen_scores" in data and "rejected_scores" in data:
                        infer_mode = "scoring"
                        break
                    elif "selected_idx" in data and "chosen_idx" in data:
                        infer_mode = "selection"
                        break
                except json.JSONDecodeError:
                    continue
        
        if infer_mode is None:
            logger.error("Could not auto-detect inference mode. Please specify --infer_mode.")
            return
    
    logger.info(f"Using inference mode: {infer_mode}")

    if infer_mode == "scoring":
        # Dictionaries to store counts for each subset
        subset_accurate_predictions = defaultdict(int)    # For traditional accuracy (all chosen > all rejected)
        subset_total_predictions = defaultdict(int)

        subset_binary_accurate_pairs = defaultdict(int)   # For binary accuracy (chosen_score > rejected_score for each pair)
        subset_total_binary_pairs = defaultdict(int)

        # Overall counts
        overall_accurate_predictions_count = 0
        overall_total_predictions = 0

        overall_binary_accurate_pairs_count = 0
        overall_total_binary_pairs_count = 0
    else:  # selection mode
        # Dictionaries to store counts for each subset
        subset_correct_selections = defaultdict(int)    # For selection accuracy
        subset_total_selections = defaultdict(int)

        # Overall counts
        overall_correct_selections = 0
        overall_total_selections = 0

    if input_length_file is not None:
        with open(input_length_file, 'r', encoding='utf-8') as lenfile:
            valid_lines = [json.loads(line)['is_valid'] for line in lenfile.readlines()]

    with open(input_file, 'r', encoding='utf-8') as infile:
        for line_num, line in enumerate(infile):
            if input_length_file is not None and not valid_lines[line_num]:
                continue

            try:
                data = json.loads(line.strip())
            except json.JSONDecodeError:
                logger.error(f"Skipping malformed JSON line {line_num + 1}: {line.strip()}")
                continue

            subset = data.get("subset", "default_subset")  # Assign to a 'default_subset' if not present

            if infer_mode == "scoring":
                chosen_scores = data.get("chosen_scores")
                rejected_scores = data.get("rejected_scores")

                if not isinstance(chosen_scores, list) or not isinstance(rejected_scores, list):
                    logger.warning(f"Line {line_num + 1}: 'chosen_scores' or 'rejected_scores' is not a list. Skipping entry.")
                    continue

                # --- Calculate Traditional Accuracy (all chosen > all rejected) ---
                is_accurate_for_entry = True
                if not chosen_scores or not rejected_scores:  # Handle empty lists
                    is_accurate_for_entry = False  # Cannot be accurate if no scores to compare
                else:
                    for c_score in chosen_scores:
                        if c_score is None:  # If any chosen score is None, this entry cannot be fully accurate
                            is_accurate_for_entry = False
                            break
                        for r_score in rejected_scores:
                            if r_score is None:  # If any rejected score is None, this entry cannot be fully accurate
                                is_accurate_for_entry = False
                                break
                            if c_score <= r_score:
                                is_accurate_for_entry = False
                                break
                        if not is_accurate_for_entry:
                            break

                # Update counts for the specific subset (traditional)
                subset_total_predictions[subset] += 1
                if is_accurate_for_entry:
                    subset_accurate_predictions[subset] += 1

                # Update overall counts (traditional)
                overall_total_predictions += 1
                if is_accurate_for_entry:
                    overall_accurate_predictions_count += 1

                # --- Calculate Binary Accuracy (chosen_score > rejected_score for each pair) ---
                current_entry_total_binary_pairs = 0
                current_entry_accurate_binary_pairs = 0

                if chosen_scores and rejected_scores:  # Only proceed if both lists are not empty
                    for c_score in chosen_scores:
                        for r_score in rejected_scores:
                            # Only compare if both scores are valid numbers (not None)
                            if c_score is not None and r_score is not None:
                                current_entry_total_binary_pairs += 1
                                if c_score > r_score:
                                    current_entry_accurate_binary_pairs += 1
                
                # Update counts for the specific subset (binary)
                subset_total_binary_pairs[subset] += current_entry_total_binary_pairs
                subset_binary_accurate_pairs[subset] += current_entry_accurate_binary_pairs

                # Update overall counts (binary)
                overall_total_binary_pairs_count += current_entry_total_binary_pairs
                overall_binary_accurate_pairs_count += current_entry_accurate_binary_pairs
            
            else:  # selection mode
                correct = int(data.get("selected_idx") == data.get("chosen_idx"))
                
                # Skip entries without a valid selection result
                if correct is None:
                    logger.warning(f"Line {line_num + 1}: No 'correct' field found. Skipping entry.")
                    continue
                
                # Update counts for the specific subset
                subset_total_selections[subset] += 1
                if correct:
                    subset_correct_selections[subset] += 1
                
                # Update overall counts
                overall_total_selections += 1
                if correct:
                    overall_correct_selections += 1


    logger.info(f"\n{'='*50}\n--- Accuracy Results from '{input_file}' ---\n{'='*50}")

    if infer_mode == "scoring":
        logger.info(f"\n--- Traditional Accuracy (Chosen > All Rejected) ---")
        for subset_name in sorted(subset_total_predictions.keys()):
            total = subset_total_predictions[subset_name]
            accurate = subset_accurate_predictions[subset_name]
            accuracy = (accurate / total) * 100 if total > 0 else 0.0
            logger.info(f"Subset '{subset_name}': Total entries: {total}, Accurate: {accurate}, Accuracy: {accuracy:.2f}%")

        overall_accuracy = (overall_accurate_predictions_count / overall_total_predictions) * 100 if overall_total_predictions > 0 else 0.0
        logger.info(f"\n--- Overall Traditional Accuracy ---")
        logger.info(f"Total entries evaluated: {overall_total_predictions}")
        logger.info(f"Accurate predictions: {overall_accurate_predictions_count}")
        logger.info(f"Overall Accuracy: {overall_accuracy:.2f}%")


        logger.info(f"\n--- Binary Accuracy (Chosen Score > Rejected Score per pair) ---")
        for subset_name in sorted(subset_total_binary_pairs.keys()):
            total_pairs = subset_total_binary_pairs[subset_name]
            accurate_pairs = subset_binary_accurate_pairs[subset_name]
            binary_accuracy = (accurate_pairs / total_pairs) * 100 if total_pairs > 0 else 0.0
            logger.info(f"Subset '{subset_name}': Total pairs: {total_pairs}, Accurate pairs: {accurate_pairs}, Binary Accuracy: {binary_accuracy:.2f}%")

        overall_binary_accuracy = (overall_binary_accurate_pairs_count / overall_total_binary_pairs_count) * 100 if overall_total_binary_pairs_count > 0 else 0.0
        logger.info(f"\n--- Overall Binary Accuracy ---")
        logger.info(f"Total pairs evaluated: {overall_total_binary_pairs_count}")
        logger.info(f"Accurate pairs: {overall_binary_accurate_pairs_count}")
        logger.info(f"Overall Binary Accuracy: {overall_binary_accuracy:.2f}%")
    
    else:  # selection mode
        logger.info(f"\n--- Selection Accuracy (Model Selected the Chosen Response) ---")
        for subset_name in sorted(subset_total_selections.keys()):
            total = subset_total_selections[subset_name]
            correct = subset_correct_selections[subset_name]
            accuracy = (correct / total) * 100 if total > 0 else 0.0
            logger.info(f"Subset '{subset_name}': Total entries: {total}, Correct selections: {correct}, Accuracy: {accuracy:.2f}%")

        overall_accuracy = (overall_correct_selections / overall_total_selections) * 100 if overall_total_selections > 0 else 0.0
        logger.info(f"\n--- Overall Selection Accuracy ---")
        logger.info(f"Total entries evaluated: {overall_total_selections}")
        logger.info(f"Correct selections: {overall_correct_selections}")
        logger.info(f"Overall Accuracy: {overall_accuracy:.2f}%")
    
    logger.info(f"\n{'='*50}\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Calculate accuracy metrics from a reward model inference output JSONL file.")
    parser.add_argument(
        "--input_file",
        type=str,
        required=True,
        help="Path to the JSONL file generated by the reward model inference script."
    )
    parser.add_argument(
        "--infer_mode",
        type=str,
        choices=["scoring", "selection"],
        default="scoring",
        help="The inference mode used to generate the results. If not provided, it will be auto-detected."
    )
    parser.add_argument(
        "--input_length_file",
        type=str,
        default=None,
        help="Optional path to a file containing validity flags for each line."
    )
    args = parser.parse_args()

    calculate_accuracy_from_file(args.input_file, args.input_length_file, args.infer_mode)
